{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Modify the code we created in class to select and print to screen the content of the blue \"Your Community\" section on https://gsm.ucdavis.edu/master-science-business-analytics-msba Use RegEx to make the text output look pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Community, Class of 2023-24, 94, Number of Incoming Students, 2, Average Years of Work Experience, 24, Average Age, 166, Average GRE Quant, Score, 3.5, Average Undergraduate GPA\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Fetching the webpage content\n",
    "page_url = 'https://gsm.ucdavis.edu/master-science-business-analytics-msba'\n",
    "response = requests.get(page_url)\n",
    "page_content = response.content\n",
    "\n",
    "# Parsing the HTML content\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# Extracting the community text\n",
    "community_text = soup.find('div', {'class': 'py-3 solid-blue-bkg text-white'}).get_text(strip=False)\n",
    "\n",
    "# Cleaning the community text using regex\n",
    "cleaned_community_text = re.sub(r'\\n+|\\xa0| {2,}', ', ', community_text).strip(', ')\n",
    "\n",
    "print(cleaned_community_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write code that web scrapes the Amazon page https://www.amazon.com/dp/B07CT6DYFG and prints its title and price to screen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Katamco The Original Toilet Timer (Classic), As Seen on Shark Tank. Funny Gift for Men, Husband, Dad, Son, Birthday, Christmas, Stocking Stuffer.\n",
      "Price: $14.99\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('https://www.amazon.com/dp/B07CT6DYFG').content, 'html.parser')\n",
    "\n",
    "item_title = soup.find(id='productTitle').get_text(strip=True)\n",
    "item_price = soup.find(\"span\",{\"class\":\"a-price\"}).find(\"span\").get_text()\n",
    "\n",
    "print(f\"Title: {item_title}\")\n",
    "print(f\"Price: {item_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write code that loads the page https://www.usnews.com/ and prints to screen the title of the current #1 top story (i.e., the title of the first title in \"Top Stories\")\n",
    "\n",
    "You will need to set the user agent in question (3). I.e., your request line should now read \"requests.get('https://www.usnews.com/', headers={'User-Agent': 'Mozilla/5.0'})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Directly navigate to the h3 element.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Found in p[Top News] next to div / div / div / div / h3\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m h3_text \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTop Stories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_next_sibling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the second top story by navigating to the sibling div of the div which contains h3 element for the first top story and finding the h3 inside it.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m h3_text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop Stories\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_next_sibling(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m               \u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/bs4/element.py:1573\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Fetching the webpage content\n",
    "response = requests.get('https://www.usnews.com/', headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Directly navigate to the h3 element.\n",
    "# Found in p[Top News] next to div / div / div / div / h3\n",
    "h3_text = soup.find('p', string=\"Top Stories\").find_next_sibling('div')[0].find('h3').get_text(strip=True)\n",
    "\n",
    "# Get the second top story by navigating to the sibling div of the div which contains h3 element for the first top story and finding the h3 inside it.\n",
    "\n",
    "# h3_text = soup.find('p', string=\"Top Stories\").find_next_sibling('div') \\\n",
    "#               .find_all('div')[2].find('h3').get_text(strip=True)\n",
    "\n",
    "\n",
    "print(h3_text)\n",
    "print(f'Top Story: {h3_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
